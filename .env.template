# ==============================
# Mem0 Deployment Lab (.env.template)
# ==============================
#
# Terraform-first lab:
# - Students do NOT create a local .env.
# - Terraform writes secrets to SSM Parameter Store.
# - EC2 user_data renders /opt/<project_name>/repo/.env from SSM at boot.
#
# Keep this file as a REFERENCE for the env vars the app supports (and for local dev if needed).

# --- Provider Selection ---
# Default track (lab): AWS Bedrock for BOTH LLM + embeddings.
# Optional track: OpenAI.
LLM_PROVIDER=aws_bedrock       # aws_bedrock | openai
EMBEDDER_PROVIDER=aws_bedrock  # aws_bedrock | openai

# --- OpenAI Configuration (ONLY if using OpenAI provider) ---
# (Terraform lab: set via terraform.tfvars; the EC2 .env is generated from SSM.)
OPENAI_API_KEY=
# Optional: custom OpenAI-compatible endpoint
OPENAI_BASE_URL=

# --- AWS Bedrock Configuration (ONLY if using aws_bedrock provider) ---
# Preferred on EC2: use the instance IAM role (no long-lived keys in env).
AWS_REGION=us-east-1
# Optional (if NOT using an IAM role):
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_SESSION_TOKEN=
# AWS_PROFILE=default

# --- Qdrant Configuration ---
# Terraform lab: Qdrant runs in Docker on the same EC2 instance network.
QDRANT_HOST=mem0_qdrant
QDRANT_PORT=6333

# --- API Configuration ---
API_HOST=0.0.0.0
API_PORT=8000
# Terraform lab: keys are generated (or provided) via Terraform and surfaced via outputs.
API_KEY=
ADMIN_API_KEY=

# --- LLM Configuration ---
# Bedrock example (default for lab):
LLM_MODEL=anthropic.claude-3-5-sonnet-20240620-v1:0
LLM_TEMPERATURE=0.7
# Optional Bedrock tuning:
# LLM_MAX_TOKENS=2000
# LLM_TOP_P=0.9
# LLM_TOP_K=1

# --- Embeddings Configuration ---
# Bedrock Titan example (default for lab):
EMBEDDER_MODEL=amazon.titan-embed-text-v1
